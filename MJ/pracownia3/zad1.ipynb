{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4111bb977b7545de8625b626ddd9e6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import functional as F\n",
    "\n",
    "model_name = 'allegro/herbert-base-cased'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' kot']\n",
      "    1.000000019482502 [ kot]\n",
      "    0.7719700332005566 [ Kot]\n",
      "    0.6805925774733181 [ koty]\n",
      "    0.6796502587651468 [ kota]\n",
      "    0.6548813907630879 [ kotów]\n",
      "    0.6197967243832898 [ kocioł]\n",
      "    0.6129579754090762 [kot]\n",
      "    0.6078859024555014 [ kotły]\n",
      "    0.6063592274362538 [ kotłowni]\n",
      "    0.6051723072278247 [ kotka]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(word):\n",
    "    ids = tokenizer(word, return_tensors='pt')['input_ids'][0]\n",
    "    return [tokenizer.decode(n) for n in ids]\n",
    "\n",
    "def cos(a, b):\n",
    "    return a.dot(b) / (a.dot(a) * b.dot(b)) ** 0.5\n",
    "\n",
    "emb = model.transformer.wte.weight.detach().cpu().numpy()\n",
    "N = 50257\n",
    "\n",
    "def find_closest(word, n=5):\n",
    "    tokens = tokenize(' ' + word)\n",
    "    print (tokens)\n",
    "    token_id = tokenizer.encode(tokens[0])[0]\n",
    "    \n",
    "    score = [(cos(emb[i], emb[token_id]), tokenizer.decode(i)) for i in range(N)]\n",
    "    score.sort(reverse=True)\n",
    "    return score[:n]\n",
    "\n",
    "for s, w in find_closest('kot', 10):\n",
    "    print ('   ', s, f'[{w}]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
