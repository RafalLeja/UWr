{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(3407)\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "062*536=062*500+062*030+062*006=031000+001860+000372=033232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 11,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 6,\n",
       " 2,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 6,\n",
       " 11,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 0,\n",
       " 10,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 7,\n",
       " 2,\n",
       " 11,\n",
       " 0,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def l_z(length, num):\n",
    "    num_str = str(num)\n",
    "    if len(num_str) < length:\n",
    "        return '0'*(length-len(num_str)) + num_str\n",
    "    return num_str\n",
    "\n",
    "def step_mul(length):\n",
    "    a = [random.randint(0,9) for i in range(length)]\n",
    "    b = [random.randint(0,9) for i in range(length)]\n",
    "\n",
    "    val_a = int(''.join(str(d) for d in a))\n",
    "    val_b = int(''.join(str(d) for d in b))\n",
    "    if val_a < val_b:\n",
    "        val_a, val_b = val_b, val_a\n",
    "        a, b = b, a\n",
    "    \n",
    "    steps = [f\"{l_z(length, val_b)}*{l_z(length, val_a)}=\"]\n",
    "    steps_eq = []\n",
    "    for i in range(length):\n",
    "        a_i = a[i] * 10**(length-i-1)\n",
    "        # a_i = str(a[i]) \n",
    "        steps.append(f\"{l_z(length, val_b)}*{l_z(length, a_i)}+\")\n",
    "        steps_eq.append(l_z(2*length, a_i*val_b)+\"+\")\n",
    "    \n",
    "    steps[-1] = steps[-1][:-1] + \"=\"\n",
    "    steps_eq[-1] = steps_eq[-1][:-1] + \"=\"\n",
    "\n",
    "    steps.extend(steps_eq)\n",
    "    steps.append(l_z(2*length, val_a*val_b))\n",
    "    out = ''.join(steps)\n",
    "    # out = list(out)\n",
    "    return out\n",
    "    # print(out)\n",
    "\n",
    "def tokenize_string(string):\n",
    "    out = []\n",
    "    for char in string:\n",
    "        if char == '+':\n",
    "            out.append(10)\n",
    "        elif char == '=':\n",
    "            out.append(11)\n",
    "        elif char == '*':\n",
    "            out.append(12)\n",
    "        else:\n",
    "            out.append(int(char))\n",
    "    return out\n",
    "\n",
    "\n",
    "eq = step_mul(3)\n",
    "print(eq)\n",
    "tokenize_string(eq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dataset for the Add problem. E.g. for problem length 3:\n",
    "    12 + 333 = 345\n",
    "    Input: 0 1 2 3 3 3 -> Output: 0 3 4 5\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 1 2 3 3 3 0 3 4\n",
    "    output: I I I I I 0 3 4 5\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=3):\n",
    "        assert split in {'train', 'test'}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000 # ...\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return 13\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer, \n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return len(tokenize_string(step_mul(self.length))) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        while True:\n",
    "            rai = tokenize_string(step_mul(self.length))\n",
    "            h = hash(str(rai[:1+2*self.length]))\n",
    "            \n",
    "            inp_split = 'test' if h % 4 == 0 else 'train' # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break # ok\n",
    "        \n",
    "        x = torch.tensor(rai[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(rai[1:], dtype=torch.long)\n",
    "        \n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[:2*self.length] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3,  7,  8, 12,  4,  9,  5, 11,  3,  7,  8, 12,  4,  0,  0, 10,  3,  7,\n",
      "         8, 12,  0,  9,  0, 10,  3,  7,  8, 12,  0,  0,  5, 11,  1,  5,  1,  2,\n",
      "         0,  0, 10,  0,  3,  4,  0,  2,  0, 10,  0,  0,  1,  8,  9,  0, 11,  1,\n",
      "         8,  7,  1,  1])\n",
      "3 -1\n",
      "7 -1\n",
      "8 -1\n",
      "12 -1\n",
      "4 -1\n",
      "9 -1\n",
      "5 11\n",
      "11 3\n",
      "3 7\n",
      "7 8\n",
      "8 12\n",
      "12 4\n",
      "4 0\n",
      "0 0\n",
      "0 10\n",
      "10 3\n",
      "3 7\n",
      "7 8\n",
      "8 12\n",
      "12 0\n",
      "0 9\n",
      "9 0\n",
      "0 10\n",
      "10 3\n",
      "3 7\n",
      "7 8\n",
      "8 12\n",
      "12 0\n",
      "0 0\n",
      "0 5\n",
      "5 11\n",
      "11 1\n",
      "1 5\n",
      "5 1\n",
      "1 2\n",
      "2 0\n",
      "0 0\n",
      "0 10\n",
      "10 0\n",
      "0 3\n",
      "3 4\n",
      "4 0\n",
      "0 2\n",
      "2 0\n",
      "0 10\n",
      "10 0\n",
      "0 0\n",
      "0 1\n",
      "1 8\n",
      "8 9\n",
      "9 0\n",
      "0 11\n",
      "11 1\n",
      "1 8\n",
      "8 7\n",
      "7 1\n",
      "1 1\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_dataset = MulDataset('train')\n",
    "test_dataset = MulDataset('test')\n",
    "x, y = train_dataset[0]\n",
    "\n",
    "print (x)\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 0.80M\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = 'gpt-micro'\n",
    "# model_config.model_type = 'gpt-nano'\n",
    "\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_242553/2938202400.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_step_85.pth', map_location='cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('model_step_85.pth'))\n",
    "\n",
    "model.load_state_dict(torch.load('model_step_85.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 128\n"
     ]
    }
   ],
   "source": [
    "print (model_config.n_head, model_config.n_layer, model_config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cpu\n"
     ]
    }
   ],
   "source": [
    "# create a Trainer object\n",
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 1e-4 # the model we're using is so small that we can go a bit faster\n",
    "train_config.max_iters = 5000\n",
    "train_config.num_workers = 0\n",
    "# train_config.batch_size = 32\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter_dt 0.00ms; iter 0: train loss 0.01482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miter_dt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_dt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1000\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mms; iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39mset_callback(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_batch_end\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_end_callback)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.config/minGPT/mingpt/trainer.py:97\u001b[0m, in \u001b[0;36mTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# backprop and update the parameters\u001b[39;00m\n\u001b[1;32m     96\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's perform some evaluation\n",
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating train batch 1/8\n",
      "evaluating train batch 2/8\n",
      "evaluating train batch 3/8\n",
      "evaluating train batch 4/8\n",
      "evaluating train batch 5/8\n",
      "evaluating train batch 6/8\n",
      "evaluating train batch 7/8\n",
      "evaluating train batch 8/8\n",
      "train final score: 343/400 = 85.75% correct\n",
      "evaluating test batch 1/8\n",
      "evaluating test batch 2/8\n",
      "evaluating test batch 3/8\n",
      "evaluating test batch 4/8\n",
      "evaluating test batch 5/8\n",
      "evaluating test batch 6/8\n",
      "evaluating test batch 7/8\n",
      "evaluating test batch 8/8\n",
      "test final score: 333/400 = 83.25% correct\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "errors = []\n",
    "\n",
    "def eval_add_split(trainer, split, max_batches):\n",
    "    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n",
    "    n = train_dataset.length # naugy direct access shrug\n",
    "    results = []\n",
    "    mistakes_printed_already = 0\n",
    "    loader = DataLoader(dataset, batch_size=50, num_workers=0, drop_last=False)\n",
    "    #loader = DataLoader(dataset, batch_size=1, num_workers=0, drop_last=False)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        print(f\"evaluating {split} batch {b+1}/{min(len(loader), max_batches)}\")\n",
    "\n",
    "        x = x.to(trainer.device)\n",
    "        y = y.to(trainer.device)\n",
    "\n",
    "        inp = x[:, :2*n+1]\n",
    "        sol = y[:, -2*n:]\n",
    "        \n",
    "        cat = model.generate(inp, 52, do_sample=False) # using greedy argmax, not sampling\n",
    "        sol_candidate = cat[:, -2*n:]      \n",
    "        # print(cat[0])\n",
    "        # print(inp[0])\n",
    "        # print(sol[0])   \n",
    "        # print(sol_candidate[0])\n",
    "        correct = (sol == sol_candidate).all(1).cpu() \n",
    "        incorrect = ~correct\n",
    "        mae = (sol - sol_candidate).abs().float().mean(1).cpu()\n",
    "        errors.extend(mae.tolist())\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "            results.append(int(correct[i]))\n",
    "    \n",
    "        if b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    rt = torch.tensor(results, dtype=torch.float)\n",
    "    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n",
    "    return rt.sum()\n",
    "\n",
    "# run a lot of examples from both train and test through the model and verify the output correctness\n",
    "with torch.no_grad():\n",
    "    train_score = eval_add_split(trainer, 'train', max_batches=8)\n",
    "    test_score  = eval_add_split(trainer, 'test',  max_batches=8)\n",
    "\n",
    "plt.hist(errors, bins=50)\n",
    "plt.xlabel('error')\n",
    "plt.ylabel('count')\n",
    "plt.title('Error histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_step_85.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
