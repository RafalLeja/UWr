{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Słownik:\n",
    "ń - ń\n",
    "ś - ś\n",
    "ę - ę\n",
    "ą - ą\n",
    "ł - ł\n",
    "ż - ż\n",
    "\\fi\n",
    "\u001c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli wykonamy zanurzenia dla korpusów w dwóch różnych językach, to wektory dla women oraz kobieta nie będą miały ze sobą związku. Zaproponuj jakiś sposób liczenia zanurzeń, w których słowa będące swoimi tłumaczeniami będą otrzymywać podobne wektory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli zrobimy dwa osobne zanurzenia dla różnych języków, i użyjemy słownika, możemy zrzutować jedno zanurzenie na drugie za pomocą np. macierzy ortonormalnej Procrustesa. W ten sposób możemy łączyć zanurzenia z różnych języków.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaproponuj jakiś sposób do generowania tekstu (ogólnie sekwencji/macierzy tokenów\n",
    "o ustalonym rozmiarze) za pomocą sieci typu BERT. Możesz wymyślić procedurę samemu, lub\n",
    "odnaleźć jakąś publikację, która takie metody opisuje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://arxiv.org/abs/1902.04094)\n",
    "\n",
    "    \n",
    "Możemy urzyć modelu BERT podając token [MASK] w miejsce słowa, które chcemy wygenerować. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Załóżmy, że dysponujemy dużym, n-gramowym modelem Mng działającym na tokenach. Zaproponuj dwa scenariusze treningu modelu typu GPT, który wykorzystuje Mng (model\n",
    "Mng może być również później wykorzystywany w inferencji, czyli w generowaniu tekstu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model Mng jako generator tekstu: Możemy użyć modelu Mng do generowania tekstu, a następnie trenować model GPT na wygenerowanym tekście.\n",
    "- Model Mng jako model języka: Możemy użyć modelu Mng do oceny prawdopodobieństwa sekwencji tokenów, a następnie trenować model GPT na sekwencjach tokenów, które model Mng ocenił jako najbardziej prawdopodobne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 4:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wracamy do zadania o Niefrasobliwym Programiście (co trenuje nie ten model, co\n",
    "trzeba). Tym razem NP trenował model typu BERT (duży i kosztowny), ale zapomniał o uwzględnieniu osadzeń pozycyjnych. Czy taki model mógłby być do czegoś użyteczny? (oczyiście od odpowiedzi zależy dalszy los NP w \u001crmie). Czy fakt, że użyty tu słownik ma bardzo dużo tokenów jest\n",
    "okolicznością sprzyjającą, czy obciążającą?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taki model może być urzyty do zadań, które nie wymagają uwzględnienia pozycji tokenów, np. do \"zanurzania\" tekstu. \n",
    "\n",
    "Użycie dużego słownika tokenów może być obciążające, ponieważ wymaga większej mocy obliczeniowej i pamięci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 5:\n",
    "Przyjmijmy, że udało się pozytywnie odpwiedzieć na pytanie w poprzednim zadaniu i\n",
    "Kierownictwo chce (dla innego języka) wytrenować inną instancję \u0012bezpozycyjnego BERT-a\u0011. Standardowy trening Maskowanego Modelu Językowego ma pewną właściwość, która jest szczególnie\n",
    "kłopotliwa, gdy nie ma osadzeń pozycyjnych. Jaka to właściwość? Wskazówka, z której lepiej nie\n",
    "korzystać (rot13.com): Cbzlśy b yvpmovr gbxraój ZNFX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 6 (*):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W klasy\u001ckacji tekstu używany jest często naiwny klasy\u001ckator bayesowski (NBC).\n",
    "Opowiedz krótko jak działa NBC, a następnie podaj co najmnie dwa powody, dlaczego dostrajanie\n",
    "BERT-a ma szansę dawać istotnie lepsze wyniki w zadaniu klasy\u001ckacji tekstu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NBC jest `naiwny` czyli nie bierze pod uwagę kolejności słów w tekście. \n",
    "2. Bert dodatkowo uwzględnia kontekst, co może poprawić wyniki klasyfikacji. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 7:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaprojektuj eksperyment, który odpowie na pytanie: \u0012czy transformery potra\u001cą sortować ciąg liczb naturalnych\u0011. Jaka jest Twoja intuicja dotycząca wyniku tego eksperymentu?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane treningowe: ciągi liczb naturalnych, rosnące lub malejące.\n",
    "\n",
    "Dane fine-tuningowe: odpowiednie prompty ze słowami kluczowymi, np. \"rosnąco\", \"malejąco\".\n",
    "\n",
    "Model: model języka, np. GPT-2.\n",
    "\n",
    "Intuicja: Model powinien nauczyć się sortować ciągi liczb naturalnych, ponieważ łatwo się nauczyć które liczby są większe, a które mniejsze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 8:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaprojektuj eksperyment, który odpowie na pytanie: \u0012czy transformery potra\u001cą obliczać wartość XOR\u0011. Jaka jest Twoja intuicja dotycząca wyniku tego eksperymentu?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dane treningowe: pary liczb naturalnych, i ich XOR, separowane przez token [SEP].\n",
    "\n",
    "Dane fine-tuningowe: odpowiednie prompty ze słowami kluczowymi, np. \"XOR\".\n",
    "\n",
    "Model: model języka, np. GPT-2.\n",
    "\n",
    "Intuicja: Model może mieć problem z nauczeniem się XOR, ponieważ nie jest to zadanie językowe, i nie ma oczywistego sposobu nauczenia się XOR z danych językowych.\n",
    "\n",
    "Poprawka: Możemy liczby konwertować na binarne, i użyć modelu do nauczenia się XOR na danych binarnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 9 (2p):\n",
    "Powiedzmy, że chcemy modelować za pomocą LM-u typu GPT (czyli autoregresywnego) kod w Pythonie. Odpowiedz na poniższe pytania:\n",
    "1. Czy standardowa, trenowana tokenizacja jest tu optymalnym wyborem? Czy też może warto\n",
    "rozważyć jakąś jej mody\u001ckację (a jeżeli tak, to jaką)\n",
    "2. Jak najlepiej obsłużyć wcięcia w kodzie?\n",
    "3. Czym jest PEP-8? Czy może on mieć jakieś użycie w tym zadaniu?\n",
    "4. Jakie są argumenty za tym, że warto zmieniać nazwy zmiennych/funkcji/klas/... w kodzie\n",
    "(zachowując jego semantykę)?\n",
    "5. Jak metody NLP (word2vec?, transformery?, ...) mogą pomóc w zamianie nazw (podaj co\n",
    "najmniej dwa przykłady)\n",
    "6. Jak statyczna analiza kodu może pomóc w tym zadaniu? (wystarczy jeden scenariusz)\n",
    "Możesz zadeklarować to zadanie za 1p, jeżeli znasz odpowiedzi na większość, ale nie wszystkie\n",
    "pytania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Nazwy funkcji popularnych bibliotek powinny być całymi tokenami\n",
    "2. Tokeny pozycyjne?\n",
    "3. PEP-8 to standard formatowania kodu w Pythonie, może być użyteczny do formatowania kodu i standardyzacji nazw zmiennych.\n",
    "4. Zmiana nazw zmiennych może poprawić czytelność kodu, i ułatwić zrozumienie kodu innym programistom.\n",
    "5. - Word2Vec może być użyty do znalezienia podobnych nazw zmiennych, i zaproponowania nowych nazw.\n",
    "   - Transformery mogą być podpowiadać nazwy zmiennych na podstawie kontekstu.\n",
    "6. Możemy sprawdzać czy wygenerowany kod jest poprawny, i czy nie ma w nim błędów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 10 (*):\n",
    "Które z pooprzednich zadań nadają się na projekt/pracę dyplomową (oczywiście,\n",
    "gdyby je odpowiednio rozbudować)? Jakie konieczne mody\u001ckacje należałoby wprowadzić (musisz\n",
    "wskazać co najmniej jedno zadanie).\n",
    "\n",
    "## Zadanie 11 (2p *):\n",
    "Zaproponuj jakieś zadanie (niezwiązane z niniejszą listą zadań), nadające się\n",
    "(Twoim zdaniem) na projekt do naszego przedmiotu. Twój opis powinien być zwięzły, niemniej\n",
    "w miarę jednoznacznie de\u001cniujący zadanie. Zgłoś je na forum na SKOSie (https://skos.ii.\n",
    "uni.wroc.pl/mod/forum/view.php?id=43738&forceview=1), postępując zgodnie z instrukcjami.\n",
    "Twoje zgłoszenie nie jest deklaracją, że chcesz to zadanie robić, mówi jedynie, że zadanie wydaje\n",
    "Ci się ciekawe i odpowiednie. Może też stać się inspiracją dla innych. Jeżeli częścią zadania jest\n",
    "trenowanie modelu, kluczowa jest wówczas kwestia opisania pozyskania danych (albo skorzystanie\n",
    "z istniejącego datasetu, albo stworzenie nowego, własnego zbioru danych)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
